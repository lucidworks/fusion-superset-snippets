# Uncomment and change this address to point to external Zookeeper or ensemble
# If you have external Zk and don't want to run Zk as part of Fusion service
# then make sure to remove "zookeeper" service from "group.default" (see below)
# default.zk.connect = localhost:9983

# Uncomment and change this address if you want to use external SolrCloud cluster
# Make sure to remove "solr" service from "group.default" (see below)
# default.solrZk.connect = localhost:2181/solr-zk-namespace

# the list of services that should be started with "bin/fusion start"
# optional services are spark-master, spark-worker, sql
group.default = zookeeper, solr, api, connectors-classic, connectors-rpc, proxy, webapps, admin-ui, sql, log-shipper, spark-master, spark-worker


# Default properties
#
# any property that begins with "default." applies to all services, unless specifically overridden in that service's
# properties. For example, if we have the setting "default.gc = cms", and then "api.gc = g1", then the API service uses
# use the G1 garbage collector, while other services use CMS (unless they, too, have gc properties specified).


# "address" is the ip / hostname that is used for inter-node or inter-component communication
# Usually this address is published to Zk so that nodes/components can discover each other
# By default it is one non localhost ip address discovered programmatically
#default.address = 127.0.0.1

# The default garbage collection options are "cms", "g1", "throughput" or "parallel", "serial" or "none". The specific
# JVM settings for each option are defined at the bottom of this file.
default.gc = cms
# enable garbage collection logs. These are logged into each service's directory, and roll over after 10 MB, with
# the 20 most recent files retained.
default.gcLog = true

# turn on or off supervision of services through the Fusion Agent. Allowed values are "standard" and "none". Set to
# "none" to disable supervision (if, for example, you are using another package for process supervision).
default.supervision.type = standard

# Agent periodically polls services to determine if they are alive and responsive. The following settings configure
# that behavior for all services. Create service-specific properties to override these properties for specific services
# e.g. api.supervision.pollingFailureCountThreshold = 3
# Note that these properties only take effect if supervision.type = standard
# Increasing either of these settings makes the system more resilient to long GC pauses, but makes it take
# longer to react to service failures.
#
# pollingFailureCountThreshold sets the number of successive failed polls that the Agent allows before declaring a
# service dead and restarting it. By default (with a value of "1"), it declares the service dead on the second
# successive failed poll.
default.supervision.pollingFailureCountThreshold = 1
# This sets the time between polls
default.supervision.pollingIntervalMillis = 1000

# Set explicit timezone for the services. (Default: UTC)
default.timezone=UTC

# The number of seconds agent will wait for a service to be up and ready before declaring failure.
default.startSecs=360

# Agent process
agent.port = 8091

# API service
api.port = 8765
api.stopPort = 7765
api.jvmOptions = -Xmx1g -Xss256k -Dhttp.maxConnections=1000

# Connectors RPC service
connectors-rpc.port = 8771
connectors-rpc.pluginPortRangeStart = 8871
connectors-rpc.pluginPortRangeEnd = 8971
connectors-rpc.jvmOptions = -Xmx1g -Xss256k -Xms512m

# Connectors service
connectors-classic.port = 8984
connectors-classic.stopPort = 7984
connectors-classic.jvmOptions = -Xmx1g -Xss256k -Dcom.lucidworks.connectors.pipelines.embedded=false

# Zookeeper
zookeeper.port = 9983
zookeeper.jvmOptions = -Xmx256m

# Solr
solr.port = 8983
solr.stopPort = 7983
solr.jvmOptions = -Xmx2g -Xss256k

# Spark master
spark-master.port = 8766
spark-master.uiPort = 8767
spark-master.jvmOptions = -Xmx512m
spark-master.envVars=SPARK_SCALA_VERSION=2.11,SPARK_PUBLIC_DNS=${default.address},SPARK_LOCAL_IP=${default.address}

# Spark worker
spark-worker.port = 8769
spark-worker.uiPort = 8770
spark-worker.jvmOptions = -Xmx1g
spark-worker.envVars=SPARK_WORKER_CORES=10,SPARK_SCALA_VERSION=2.11,SPARK_PUBLIC_DNS=${default.address},SPARK_LOCAL_IP=${default.address}

# Admin UI
admin-ui.port = 8763
admin-ui.stopPort = 7763
admin-ui.jvmOptions = -Xmx512m

# Proxy
proxy.port = 8764
proxy.stopPort = 7764
proxy.jvmOptions = -Xmx512m
# Make sure to enable ssl mode if you disable http access in jetty
# see https://doc.lucidworks.com/fusion-server/latest/system-administration/security/ssl.html#enabling-ssl-in-the-fusion-ui
# proxy.ssl=false
# Takes a regular expression defining which origins Fusion allows access from, using CORS standard.
proxy.corsAllowOrigin = .*

# SQL engine
sql.port = 8768
sql.jvmOptions = -Xmx1g
sql.envVars=SPARK_SCALA_VERSION=2.11,SPARK_PUBLIC_DNS=${default.address},SPARK_LOCAL_IP=${default.address}

# Webapps Server
webapps.port = 8780
webapps.stopPort = 7780
webapps.jvmOptions = -Xmx512m

# you can limit this Webapps service instance to deploy only particular apps
# webapps.appIds = app1, app2

# register web applications for custom / selective routing
webapps.routing_group=webapp

# Log Shipper
# monitoring port
log-shipper.port = 8781
# bind to localhost only
log-shipper.address = 127.0.0.1

# Uncomment and change this address if you want to index log messages
# into another externally manageable cluster
# log-shipper.solrZk.connect = localhost:2181/solr-zk-namespace

# Change collection name to index log messages to any custom Solr collection
# Make sure that this collection exists
log-shipper.solrCollection = system_logs

# Alternatively if you want to send logs to Solr HTTP endpoint without Zk access
# specify Solr http address instead
# log-shipper.solrHttp = http://localhost:8983/solr


# You can ship Fusion logs of three different types:
# 1. java - java logs
# 2. http - HTTP access logs from web servers
# 3. gc - garbage collection logs
log-shipper.logs.proxy = java, http
log-shipper.logs.zookeeper = java
log-shipper.logs.api = java
log-shipper.logs.connectors = java
log-shipper.logs.admin-ui = java
log-shipper.logs.webapps = java
log-shipper.logs.spark-master = java
log-shipper.logs.spark-worker = java
log-shipper.logs.agent = java
# DO NOT ship Solr logs unless you enabled shipping to external Solr cluster (see log-shipper.solrZk.connect above)
# log-shipper.logs.solr = java, http

log-shipper.jvmOptions = -Xms256m -Xmx256m
log-shipper.supervision.type = none


# these define the default garbage collection options. You may define your own configurations as you wish: to define a
# new GC option named "custom", for example, create a property named "gcOptions.custom" and set the value to whatever
# you would like added to the JVM command line.
gcOptions.cms = -XX:NewRatio=3 -XX:SurvivorRatio=4 -XX:TargetSurvivorRatio=90 -XX:MaxTenuringThreshold=8 -XX:+UseConcMarkSweepGC \
  -XX:+UseParNewGC -XX:ConcGCThreads=4 -XX:ParallelGCThreads=4 -XX:+CMSScavengeBeforeRemark \
  -XX:PretenureSizeThreshold=64m -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=50 \
  -XX:CMSMaxAbortablePrecleanTime=6000 -XX:+CMSParallelRemarkEnabled -XX:+ParallelRefProcEnabled
gcOptions.cms.java7 = -XX:CMSFullGCsBeforeCompaction=1 -XX:CMSTriggerPermRatio=80
gcOptions.g1 = -XX:+UseG1GC -XX:+ParallelRefProcEnabled -XX:MaxGCPauseMillis=250 -XX:+AggressiveOpts -XX:InitiatingHeapOccupancyPercent=75
gcOptions.throughput = -XX:+UseParallelGC -XX:+ParallelRefProcEnabled -XX:MaxGCPauseMillis=250
gcOptions.serial = -XX:+UseSerialGC
gcOptions.none=
cat: cat: No such file or directory
<?xml version="1.0"?>
<configuration>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:derby:memory:databaseName=metastore_db;create=true</value>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.apache.derby.jdbc.EmbeddedDriver</value>
  </property>

  <property>
    <name>hive.server2.authentication</name>
    <value>CUSTOM</value>
  </property>

  <!-- To enable Fusion based authentication / authorization -->
  <property>
    <name>hive.server2.custom.authentication.class</name>
    <value>com.lucidworks.spark.FusionHiveAuthenticationProvider</value>
  </property>

  <property>
    <name>hive.server2.session.hook</name>
    <value>com.lucidworks.spark.FusionSessionHook</value>
  </property>

  <property>
    <name>hive.server2.transport.mode</name>
    <value>binary</value>
  </property>

  <property>
     <name>hive.server2.thrift.port</name>
     <value>8768</value>
 </property>

  <property>
    <name>hive.metastore.sasl.enabled</name>
    <value>false</value>
  </property>

 <property>
    <name>hive.exec.scratchdir</name>
    <value>/Users/marcussorealheis/Documents/fusion/var/sql/tmp</value>
  </property>

  <property>
    <name>hive.scratch.dir.permission</name>
    <value>733</value>
  </property>
</configuration>
